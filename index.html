<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-tw">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Machine Learning, Deep Learning, Graph Neural Network, Natural Language Processing">
<meta property="og:type" content="website">
<meta property="og:title" content="Hui-Yu Huang&#39;s Blog">
<meta property="og:url" content="https://purelyvivid.github.io/index.html">
<meta property="og:site_name" content="Hui-Yu Huang&#39;s Blog">
<meta property="og:description" content="Machine Learning, Deep Learning, Graph Neural Network, Natural Language Processing">
<meta property="og:locale" content="zh-tw">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hui-Yu Huang&#39;s Blog">
<meta name="twitter:description" content="Machine Learning, Deep Learning, Graph Neural Network, Natural Language Processing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://purelyvivid.github.io/">





  <title>Hui-Yu Huang's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hui-Yu Huang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">To Live is to Wonder</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            歸檔
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://purelyvivid.github.io/2019/07/07/GCN_1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hui-Yu Huang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar_2016.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui-Yu Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/07/GCN_1/" itemprop="url">GCN(Graph Convolutional Network)的理解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-07T12:00:00+08:00">
                2019-07-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在深度學習領域的人，對<strong>捲積</strong>(Convolution)應該都不陌生，但不一定聽說過<strong>圖</strong>(Graph)也可以做Convolution。這篇文章對<strong>GCN</strong>(Graph Convolutional Network)做了概略的介紹。</p>
<hr>
<p>CNN的捲積不是數學定義上的連續捲積，而是一種定義的離散捲積。我們用這種捲積來處理圖像(image)，從圖像中提取特徵，並且透過神經網路來學習捲積的權重(weight)。以下所稱的捲積都是指這種局部的離散捲積。</p>
<p><img src="https://pic4.zhimg.com/80/v2-394cb7b5f6dfb23dcddd838ebdee548b_hd.jpg" alt></p>
<p>CNN捲積有一些特性，</p>
<p>(1)平移不變性(shift-invariance)<br>(2)局部性(local connectivity)<br>(3)多尺度(multi-scale)</p>
<p>這些特性，暫且把它們稱作<strong>組合性(Compositionality)</strong>，讓CNN捲積只能處理在歐幾里德空間(Euclidean Structure)的數據。</p>
<p><img src="https://cdn-images-1.medium.com/max/1200/1*1VJDP6qDY9-ExTuQVEOlVg.gif" alt></p>
<p>什麼數據是Euclidean Structure的數據呢？最經典的就是影像(image)，影片(video)和音頻(speech/voice)，自然語言文本(text)透過特殊處理，也可以固定維度，投影到歐幾里德空間來操作。</p>
<p>然而，有一些數據結構是<strong>Non-Euclidean</strong>的，例如:</p>
<ul>
<li>生物/化學分子 Chemical/ Biological Graphs</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-4a3967e884e413c9d0c5e10af79884c7_hd.jpg" alt></p>
<ul>
<li>Citation Networks / Social Networks </li>
</ul>
<p>![](<a href="https://i.imgur.com/gH7YZra.jpg" target="_blank" rel="noopener">https://i.imgur.com/gH7YZra.jpg</a> =500x)</p>
<ul>
<li>Unstructured Graphs: 例如3D點雲, 或者是圖像的半監督學習</li>
</ul>
<p><img src="http://paradise.caltech.edu/~yli/software/pceditor/3dselect2.png" alt></p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*IPmyksaiYuo0fflQSUB4BQ.png" alt></p>
<p>Non-Euclidean的數據，大致上可以當作<strong>圖</strong>(Graph)來理解。也就是可以理解成$G=(V,E)$，由頂點(vertex)和邊(edge)所構成的一種資料格式。</p>
<p><strong>Euclidean數據可以當作是Non-Euclidean的一種特例</strong>。因此對Non-Euclidean理解和處理，可以在Euclidean數據上通用，這就是為什麼Non-Euclidean數據的表示法學習有其重要性。</p>
<p>![](<a href="https://i.imgur.com/VVYfDyx.png" target="_blank" rel="noopener">https://i.imgur.com/VVYfDyx.png</a> =500x)</p>
<p>對2D Convolution來說，Graph Convolution的想法來自於把在<strong>圖像域</strong>(image domain)運作得很好的2D捲積神經網路方法，拿到<strong>圖域</strong>(graph domain)來使用。</p>
<hr>
<p>要了解圖的Convolution，要先了解<strong>Convolution本質上是一種aggregation操作</strong>，它是一種局部加權平均的運算。</p>
<p>對圖的頂點(vertex)來說，局部指的是它的鄰居，而它的鄰居由邊(edge)的存在與否，綜合邊的權重(weight)大小去定義。簡單起見，把有邊(edge)的權重都定為$1$，沒邊(edge)的權重都定為$0$。</p>
<p>為了探討圖的性質，我們用<strong>度矩陣</strong>(degree matrix)$D$ 和 <strong>鄰居矩陣</strong>(adjacency matrix)$A$來表示一個圖。度矩陣是一個對角矩陣(diagonal matrix)，度矩陣對角線上的值就是該頂點連接著幾條邊，也就是鄰居矩陣的<strong>列和</strong>。</p>
<p>假如是無向圖，鄰居矩陣是對稱矩陣(symmetric matrix)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">D = np.sum(A, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://i.imgur.com/sJHABf0.png" alt></p>
<p>另外圖論(graph theory)還定義一個東西叫做<strong>Laplacian Matrix</strong> $L$，它最簡單的定義是$D-A$。在數學及物理中，Laplacian或稱Laplace operator(拉普拉斯算子)是歐幾里得空間中的一個函數的<strong>梯度的散度</strong>給出的微分算子，通常寫成 $\Delta$、$\nabla^2$ 或 $\nabla \cdot \nabla$。可以粗糙的視為歐幾里得空間中二次微分操作。</p>
<p><a href="https://www.zhihu.com/question/54504471/answer/630639025" target="_blank" rel="noopener">Laplacian物理上的解釋可以是能量的流失或是物質擴散</a>，放在圖論上來說，即是該頂點<strong>信息</strong>(message)<strong>傳播</strong>(propagation)。</p>
<p>圖卷積神經網路(GCN)的<strong>單層</strong>操作，則可以(但不一定要)定義成：<br>$$Y=LX$$</p>
<p>因為圖的Laplacian Matrix最簡單的定義是$L=D-A$：<br>$$Y=LX=DX-AX$$</p>
<p>對單一節點來說，式中的 $AX$ 可以視為本次操作(每單位時間)鄰居預計要從我身上拿走的訊息量。 $DX$ 可以視為是本次操作<strong>每個節點尚未進行傳播前，本來擁有的訊息量</strong>，即是<strong>上次操作</strong>(上一個單位時間)每個頂點從它的鄰居頂點取得的訊息量。兩者相減，就是本次操作後訊息傳播的狀況。</p>
<p>我們用程式碼來了解一下圖的訊息傳遞：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定義鄰居矩陣A，故意使它不對稱，有向圖比較容易看到訊息傳遞的狀況</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.matrix([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line">    dtype=float</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 畫圖程式(optional)</span></span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">G=nx.DiGraph(A)</span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://i.imgur.com/lWAHzMi.png" alt><br>這個有向圖可以理解為，每次的$L$操作，每個節點會依照箭頭指出的方向去提供每個鄰居節點它所要求攜帶的訊息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定義初始特徵</span></span><br><span class="line">X = np.matrix([[ <span class="number">100.</span>, <span class="number">-100.</span>],</span><br><span class="line">                [ <span class="number">1.</span>, <span class="number">-1.</span>],</span><br><span class="line">                [ <span class="number">2.</span>, <span class="number">-2.</span>],</span><br><span class="line">                [ <span class="number">3.</span>, <span class="number">-3.</span>]])</span><br><span class="line"></span><br><span class="line">A * X</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#輸出：</span><br><span class="line">matrix([[   1.,   -1.],</span><br><span class="line">        [   5.,   -5.],</span><br><span class="line">        [   0.,    0.],</span><br><span class="line">        [ 102., -102.]])</span><br></pre></td></tr></table></figure>

<p>從以上例子就可以明顯看出，因為label=3的節點提供訊息給label=0和label=2的節點，結果得到一個很大的訊息更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">D = np.diag(np.array(np.sum(A, <span class="number">1</span>)).flatten())</span><br><span class="line">L = D-A</span><br><span class="line">L*X</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#輸出：</span><br><span class="line">matrix([[ 99., -99.],</span><br><span class="line">        [ -3.,   3.],</span><br><span class="line">        [  0.,   0.],</span><br><span class="line">        [-96.,  96.]])</span><br></pre></td></tr></table></figure>

<p>結果label=3的節點更新完後，有了很大的變化。</p>
<p>我們這時候應該也發現了一些問題。由於圖中沒有自環，訊息只能被鄰居不斷取走，而沒有因應自身訊息做調整。這就像是預估一個人薪水多高時，只考慮他的朋友薪水多高，而沒有把該人的自身條件考慮進去，顯然不太合理。因此，因應任務需要，可以考慮增加自環。</p>
<p>$$\hat{A} = A+I$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加自環</span></span><br><span class="line">I = np.diag([<span class="number">1</span>]*A.shape[<span class="number">0</span>])</span><br><span class="line">A_hat = A + I</span><br><span class="line">A_hat</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#輸出：</span><br><span class="line">matrix([[1., 1., 0., 0.],</span><br><span class="line">        [0., 1., 1., 1.],</span><br><span class="line">        [0., 0., 1., 0.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A_hat * X</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#輸出：</span><br><span class="line">matrix([[ 101., -101.],</span><br><span class="line">        [   6.,   -6.],</span><br><span class="line">        [   2.,   -2.],</span><br><span class="line">        [ 105., -105.]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">D_hat = np.diag(np.array(np.sum(A, <span class="number">1</span>)).flatten())</span><br><span class="line">L_hat = D_hat-A_hat</span><br><span class="line">L_hat*X</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#輸出：</span><br><span class="line">matrix([[ -1.,   1.],</span><br><span class="line">        [ -4.,   4.],</span><br><span class="line">        [ -2.,   2.],</span><br><span class="line">        [-99.,  99.]])</span><br></pre></td></tr></table></figure>

<p>以上的傳播矩陣，不管是$A$還是$\hat{A}$還是$L$，與convolution相似的地方在於，它們都是一種<strong>局部的aggregation操作</strong>。然而，convolution的局部連接數是固定的，這裡卻不是這樣，每個頂點鄰居數目可能都不同。</p>
<p>所以，不管是$A$還是$\hat{A}$還是$L$，都還需要進行歸一化處理。以免偏好鄰居較多的節點，讓鄰居較多的節點傳播比較多的訊息量。歸一化處理的方法有兩種：(1)算術平均(2)幾何平均</p>
<p>於是多定義了兩種<strong>Laplacian Matrix</strong><br>(1)算術平均<br>$$L^{rw}=D^{-1}L$$<br>(2)幾何平均<br>$$L^{sym}=D^{-0.5}LD^{-0.5}$$ </p>
<p>其中，幾何平均受極端值影響較小，$L^{sym}$是GCN中較為常用的 Laplacian Matrix 。</p>
<p>故，圖卷積神經網路(GCN)的<strong>單層</strong>操作，也可以(但不一定要)定義成：<br>$$Y=L^{sym}X$$</p>
<hr>
<h2 id="Eigendecomposition-Spectral-decomposition-of-Laplacian-Matrix"><a href="#Eigendecomposition-Spectral-decomposition-of-Laplacian-Matrix" class="headerlink" title="Eigendecomposition / Spectral decomposition of Laplacian Matrix"></a>Eigendecomposition / Spectral decomposition of Laplacian Matrix</h2><p>假設為無向圖，此時$L^{sym}$ (以下簡稱$L$) 是可以被對角化的 (原因不詳述)</p>
<p><img src="https://www.zhihu.com/equation?tex=+L%3D+U%5Cleft%28%5Cbegin%7Bmatrix%7D%5Clambda_1+%26+%5C%5C%26%5Cddots+%5C%5C+%26%26%5Clambda_n+%5Cend%7Bmatrix%7D%5Cright%29+U%5E%7B-1%7D" alt></p>
<p>由於$U$是正交矩陣，$UU^T=I$，$U^T=U^{-1}$</p>
<p><img src="https://www.zhihu.com/equation?tex=L%3D+U%5Cleft%28%5Cbegin%7Bmatrix%7D%5Clambda_1+%26+%5C%5C%26%5Cddots+%5C%5C+%26%26%5Clambda_n+%5Cend%7Bmatrix%7D%5Cright%29+U%5E%7BT%7D" alt></p>
<p>其中，$U$是已知的，$\lambda_l$ <strong>特徵值</strong>是未知要求取的。但我們並不直接去求或是去優化$\lambda_l$ 。</p>
<p>對一個捲積函數 $h$ 而言，可以用<strong>傅立葉轉換的卷積定理</strong><br><img src="https://www.zhihu.com/equation?tex=%28f%2Ah%29_G%3DU%28%28U%5ETh%29%5Codot%28U%5ETf%29%29+%5Cqquad%282%29" alt></p>
<p>去證明(原因不詳述)，捲積後的結果為：</p>
<p><img src="https://www.zhihu.com/equation?tex=%28f%2Ah%29_G%3D+U%5Cleft%28%5Cbegin%7Bmatrix%7D%5Chat+h%28%5Clambda_1%29+%26+%5C%5C%26%5Cddots+%5C%5C+%26%26%5Chat+h%28%5Clambda_n%29+%5Cend%7Bmatrix%7D%5Cright%29+U%5ETf+%5Cqquad%281%29" alt></p>
<p>其中$\hat{h}(\lambda_{l})$要設計成:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bh%7D%28%5Clambda_l%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Bh%28i%29+u_l%5E%2A%28i%29%7D" alt><br>其中$u^{<em>}$是*</em>傅立葉轉換正交特徵向量**的共軛轉置</p>
<h2 id="第一代GCN"><a href="#第一代GCN" class="headerlink" title="第一代GCN"></a>第一代GCN</h2><p><a href="https://arxiv.org/abs/1312.6203" target="_blank" rel="noopener">Spectral Networks and Locally Connected Networks on Graphs(2013)</a></p>
<p>直接令 $\theta_l=\hat{h}(\lambda_{l})$ </p>
<p>GCN的<strong>單層</strong>操作:</p>
<p><img src="https://www.zhihu.com/equation?tex=+y_%7Boutput%7D%3D%5Csigma+%5Cleft%28U+g_%5Ctheta%28%5CLambda%29+U%5ET+x+%5Cright%29+%5Cqquad%283%29" alt><br><img src="https://www.zhihu.com/equation?tex=+g_%5Ctheta%28%5CLambda%29%3D%5Cleft%28%5Cbegin%7Bmatrix%7D%5Ctheta_1+%26%5C%5C%26%5Cddots+%5C%5C+%26%26%5Ctheta_n+%5Cend%7Bmatrix%7D%5Cright%29" alt></p>
<p>第一代圖捲積跟一般CNN卷積的差異：</p>
<p>(1) 卷積核(kernel)只在主對角線上，並不需要是二維的，卷積核個數與頂點數(n)相同<br>(2) 卷積核(kernel)在主對角線上的矩陣需要經過離散傅立葉矩陣轉換($U$,$U^T$)</p>
<h2 id="第二代GCN"><a href="#第二代GCN" class="headerlink" title="第二代GCN"></a>第二代GCN</h2><p><a href="http://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering" target="_blank" rel="noopener">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering(2016)</a></p>
<p>設計：<br>$\hat{h}(\lambda_{l})$=<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%3D0%7D%5EK+%5Calpha_j+%5Clambda%5Ej_l" alt><br>K是K-hop neighbor</p>
<p>可以導出GCN的<strong>單層</strong>操作（推導過程略）:<br><img src="https://www.zhihu.com/equation?tex=+y_%7Boutput%7D%3D%5Csigma+%5Cleft%28+%5Csum_%7Bj%3D0%7D%5EK+%5Calpha_j+L%5Ej+x+%5Cright%29+%5Cqquad%285%29" alt><br>卷積核(kernel)是$\alpha_j$</p>
<p>第二代圖捲積跟第一代圖捲積的差異：</p>
<p>(1) 第一代沒有局部性，卷積核個數與頂點數(n)相同，第二代只有K個參數，計算複雜度降低很多</p>
<p>(2) 第一代每次向前傳播都要計算一次 $U\ diag(\theta)\ U^T$ 三者的乘積，第二代直接拿$L$乘上$\alpha_j$，計算簡單一些，但複雜度仍是$O(n^2)$</p>
<p>(3) 第二代有Parameter Sharing的部份</p>
<hr>
<p>對GCN的介紹暫時到這邊，有空再來分享paper和程式碼。實際上程式碼並沒有理論來的那麼複雜，用numpy自己刻一個是絕對可行的，假如用deep learning套件，實際上也只有用到fc layer。基於PyTorch有非常好用且完整的套件<strong>torch_geometric</strong>，雖無法handle大圖，但拿來上手可以很快進入狀況。</p>
<hr>
<p>轉載請附上來源連結</p>
<hr>
<p>Reference:</p>
<ul>
<li><p>A Comprehensive Survey on Graph Neural<br>Networks <a href="https://arxiv.org/pdf/1901.00596.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.00596.pdf</a></p>
</li>
<li><p>如何理解 Graph Convolutional Network（GCN）？ <a href="https://www.zhihu.com/question/54504471" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471</a></p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://purelyvivid.github.io/2019/07/05/BPalgo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hui-Yu Huang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar_2016.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui-Yu Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/05/BPalgo/" itemprop="url">BP Algorithm的理解思路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-05T12:00:00+08:00">
                2019-07-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文淺顯直白的介紹反向傳播算法（BP 算法, The Back Propagation Algorithm）的理解思路, 計算過程和程式實現</p>
<hr>
<p>我們考慮一個最簡單的layer,這個layer有兩個input $x_1, x_2$ 和1個output $y_1$:<br><img src="https://i.imgur.com/eRqdjfG.png" alt></p>
<p>則數學式可以寫成:<br>$$ y_1 = w_{1}x_1+w_{2}x_2$$</p>
<p>上式的$w$下標是跟著$x$在跑, 考慮到$y$可以擴展成$y_j$(不只一個$y$), 我們改一下$w$的下標, 也把$y$的標號考慮進去:<br>$$ y_1 = w_{11}x_1+w_{12}x_2$$</p>
<p>其中$w_{ji}$的第一個下標$j$是跟著$y$在跑, 而$i$是跟著$x$在跑, 整個式子可以寫成矩陣形式:<br>$$Y=WX$$</p>
<p>其中令輸入$X$有$n$維, 輸出$Y$有$m$維:</p>
<p>$$X=[x_1,\ x_2, … , x_i,\ …\ ,\ x_n\ ]^T$$</p>
<p>$$Y=[y_1,\ y_2, … , y_j,\ …\ ,\ y_m\ ]^T$$</p>
<p>$$ W=<br>{\left[ \begin{array}{ccc}<br>w_{11},\ w_{12}, … ,\ w_{1n}\ \<br>… , w_{ji}, … \<br>w_{m1},\ w_{m2}, … ,\ w_{mn}\ \<br>\end{array}\right ]} = { w_{ji} }_{m \times n}$$</p>
<p>通常我們還會把上式加個<strong>偏置</strong>(bias) $B$:</p>
<p>$$Y=WX+B$$</p>
<p>$B$維度會和$Y$相同<br>$$B=[b_1,\ b_2, … , b_j,\ …\ ,\ b_m\ ]^T$$</p>
<p>通常還會再加個可微分的激活函數$\sigma$來增加其非線性:</p>
<p>$$Y=\sigma(WX+B)$$</p>
<p>以上是”前饋”(forward)的部分</p>
<hr>
<p>接下來考慮”反向傳播”(backward)的部分,假設損失函數(loss function)$\mathbb{J}$ 有$d$維, 下標用$k$表示:</p>
<p>$$ Assume\ lost function\ \mathbb{J}\ has\ dim\ d $$</p>
<p>$$\mathbb{J}=[J_1,\ J_2, … , J_k,\ …\ ,\ J_d\ ]^T$$</p>
<p>損失梯度(Gradient)要從輸出$Y$流回輸入$X$, 並流到$W$和$B$以進行權重和偏置的更新(update), 也就是可以把問題定義成:</p>
<p>$$ Given\  \frac{\partial J_k}{\partial y_{j}} , find\ \frac{\partial J_k}{\partial w_{ji}},\ \frac{\partial J_k}{\partial b_j} , and\ \frac{\partial J_k}{\partial x_i}   $$</p>
<p>梯度傳遞會用到微積分的<strong>鏈鎖率</strong>(Chain Rule), 為了方便起見我們多定義一個$Z$:</p>
<p>$$ Z = WX+B $$</p>
<p>$$ Y = \sigma (Z) $$</p>
<p>$$Z=[z_1,\ z_2, … , z_j,\ …\ ,\ z_m\ ]^T$$</p>
<p>可以發現, 上式和原本的$Y=\sigma(WX+B)$並沒有不同, 只是計算過程中間多一個$Z$</p>
<p>定義輸出梯度(已知):<br>$$\nabla_{out} = \frac{\partial J_k}{\partial y_{j}}$$</p>
<p>定義輸入梯度(待求):<br>$$\nabla_{in} = \frac{\partial J_k}{\partial x_{i}}$$</p>
<p>用鏈鎖率(Chain Rule)將待求的項目展開:</p>
<p>$$ \frac{\partial J_k}{\partial w_{ji}} = \frac{\partial J_k}{\partial y_{j}} \frac{\partial y_j}{\partial z_j}  \frac{\partial z_j}{\partial w_{ji}} = G_{kj}\ \frac{\partial z_j}{\partial w_{ji}} \ \ …(1)$$</p>
<p>$$ \frac{\partial J_k}{\partial b_{j}} =  \frac{\partial J_k}{\partial y_{j}} \frac{\partial y_j}{\partial z_j}  \frac{\partial z_j}{\partial b_{j}}  = G_{kj}\ \frac{\partial z_j}{\partial b_{j}}  \ \ …(2)$$ </p>
<p>$$ \frac{\partial J_k}{\partial x_{i}}\ (\nabla_{in}) =  \frac{\partial J_k}{\partial y_{j}} \frac{\partial y_j}{\partial z_j}  \frac{\partial z_j}{\partial x_{i}}  = G_{kj}\ \frac{\partial z_j}{\partial x_{i}}  \ \ …(3)$$ </p>
<p>上列各式重複$\frac{\partial J_k}{\partial y_{j}} \frac{\partial y_j}{\partial z_j}$的部分, 方便起見, 定義中介梯度$\mathbb{G}$:</p>
<p>$$ \mathbb{G}\ =  \frac{\partial J_k}{\partial y_{j}} \frac{\partial y_j}{\partial z_j}  $$ </p>
<p>$$ \mathbb{G}=<br>{\left[ \begin{array}{ccc}<br>G_{11},\ G_{12}, … ,\ G_{1m}\ \<br>… , G_{kj}, … \<br>G_{k1},\ G_{k2}, … ,\ G_{dm}\ \<br>\end{array}\right ]} ={ G_{kj}}_{d \times m}$$</p>
<p>至此, 整個計算流程(計算圖)已經很明朗了,<br>我們的目的就是找到:</p>
<p>$$ Find\ \ \mathbb{G},\  \frac{\partial z_j}{\partial w_{ji}} ,\  \frac{\partial z_j}{\partial b_{j}} ,\  \frac{\partial z_j}{\partial x_{i}} $$ </p>
<p>只要算出上述四項, 就可以得到所求</p>
<ol>
<li>計算$G_{kj}$</li>
</ol>
<p>$$  \frac{\partial y_j}{\partial z_j}  = \sigma ‘ (z_j) $$ </p>
<p>$$ =&gt; G_{kj} = \frac{\partial J_k}{\partial y_{j}}\ \sigma ‘ (z_j) = (dout)\ \sigma ‘ (z_j)$$ </p>
<ol start="2">
<li>計算$\frac{\partial z_j}{\partial w_{ji}} ,\  \frac{\partial z_j}{\partial b_{j}} ,\  \frac{\partial z_j}{\partial x_{i}}$</li>
</ol>
<p>$$  \frac{\partial z_j}{\partial w_{ji}}  = x_{i} $$ </p>
<p>$$  \frac{\partial z_j}{\partial b_{j}}  = 1 $$ </p>
<p>$$  \frac{\partial z_j}{\partial x_{i}}  = w_{ji} $$ </p>
<ol start="3">
<li>代入式(1)(2)(3), 得到結果</li>
</ol>
<p>$$ =&gt; \frac{\partial J_k}{\partial w_{ji}} = G_{kj} \ x_i\ ,\ \ \frac{\partial J_k}{\partial b_{j}} = G_{kj} , \ \ \frac{\partial J_k}{\partial x_{i}}\ (\nabla_{in}) =  G_{kj}\ w_{ji}\ $$</p>
<hr>
<p>以上就是整個BP演算法”單層”的計算過程, 化成python程式碼如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,dout)</span>:</span>  </span><br><span class="line">    Z,x = self.cache</span><br><span class="line">    G = dout * self.dactive_fn(Z)</span><br><span class="line">    din = G.dot(self.W.T)</span><br><span class="line">    dW = x.T.dot(G) </span><br><span class="line">    db = G.sum(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> din,dW,db</span><br></pre></td></tr></table></figure>

<p>程式碼來源: <a href="https://github.com/purelyvivid/DeepLearning_practice/blob/master/3.%20BP%20algo.py" target="_blank" rel="noopener">https://github.com/purelyvivid/DeepLearning_practice/blob/master/3.%20BP%20algo.py</a> (用numpy寫一個BP algorithm)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar_2016.jpeg" alt="Hui-Yu Huang">
            
              <p class="site-author-name" itemprop="name">Hui-Yu Huang</p>
              <p class="site-description motion-element" itemprop="description">Machine Learning, Deep Learning, Graph Neural Network, Natural Language Processing</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">文章</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/purelyvivid" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:purelyvivid@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui-Yu Huang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 強力驅動</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主題 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
